{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import necessary tools\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "# train_test_split split the dataframe into training and testing data\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# CountVectorizer turns the text into a bag-of-words vectors\n",
    "# each tokens acts as a feature for the ML classification problem\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# importing the naive bayes model class MultinomialNB (NaiveBayes),\n",
    "# which works well with count_vectorizers as it expects integer inputs\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "# metric model to evaluate the model performance\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load data into DataFrame\n",
    "comments = pd.read_csv('attack_annotated_comments.tsv', sep='\\t', index_col=0)\n",
    "annotations = pd.read_csv('attack_annotations.tsv', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 115864 unique rev_id\n"
     ]
    }
   ],
   "source": [
    "# print the # of unique rev_id\n",
    "print('There are', len(annotations['rev_id'].unique()), 'unique rev_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# labels a comment as an attack if the majority of annotators did so\n",
    "labels = annotations.groupby('rev_id')['attack'].mean() > 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# insert column 'labels' in DataFrame 'comments'\n",
    "comments['attack'] = labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Parsing: remove \"NEWLINE_TOKEN\" and \"TAB_TOKEN\" tokens from 'comment'\n",
    "comments['comment'] = comments['comment'].apply(lambda x: x.replace(\"NEWLINE_TOKEN\", \" \"))\n",
    "comments['comment'] = comments['comment'].apply(lambda x: x.replace(\"TAB_TOKEN\", \" \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is your DataFrame:\n",
      "\n",
      "                                                  comment  year  logged_in  \\\n",
      "rev_id                                                                       \n",
      "37675   `- This is not ``creative``.  Those are the di...  2002      False   \n",
      "44816   `  :: the term ``standard model`` is itself le...  2002      False   \n",
      "49851     True or false, the situation as of March 200...  2002      False   \n",
      "89320    Next, maybe you could work on being less cond...  2002       True   \n",
      "93890                This page will need disambiguation.   2002       True   \n",
      "\n",
      "             ns  sample  split  attack  \n",
      "rev_id                                  \n",
      "37675   article  random  train   False  \n",
      "44816   article  random  train   False  \n",
      "49851   article  random  train   False  \n",
      "89320   article  random    dev   False  \n",
      "93890   article  random  train   False  \n"
     ]
    }
   ],
   "source": [
    "print('This is your DataFrame:\\n')\n",
    "print(comments.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "These is the head of the comments classified as an attack\n",
      "\n",
      "rev_id\n",
      "801279             Iraq is not good  ===  ===  USA is bad   \n",
      "2702703      ____ fuck off you little asshole. If you wan...\n",
      "4632658         i have a dick, its bigger than yours! hahaha\n",
      "6545332      == renault ==  you sad little bpy for drivin...\n",
      "6545351      == renault ==  you sad little bo for driving...\n",
      "Name: comment, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print('These is the head of the comments classified as an attack\\n')\n",
    "print(comments.query('attack')['comment'].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create y which is the outcome label the model has to learn\n",
    "y = comments['attack']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we split the dataframe into training and testing data.\n",
    "split the features ('comment' column) and the label y ('attack'column) based on a given test size such as 0.33 (33%)\n",
    "\n",
    "The function will take 33% of rows to be marked as test data and move them from the training data.\n",
    "The test data is later used to see how the model has learned\n",
    "\n",
    "The resulting data from train_test_split() are:\n",
    "\n",
    "training data as X_train, \n",
    "training labels as y_train,\n",
    "testing data as X_test and\n",
    "testing labels as y_test\n",
    "\n",
    "(We use random state, so we can have repeatable results when we run the code again)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(comments['comment'], y, test_size=0.33, random_state=53)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create count vectorizer that turn the text into a bag-of-words vectors.\n",
    "Each tokens acts as a feature for the machine learning classification problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "count_vectorizer = CountVectorizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "fit transform on the training data creates a bag-of-words vectors\n",
    "It will generate a mapping of words with IDs and vectors\n",
    "representing how many times words appears in the comment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "count_train = count_vectorizer.fit_transform(X_train.values)\n",
    "count_test = count_vectorizer.transform(X_test.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it is time to build the Naive Bayes classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# class inizialization  and fit calling on training data\n",
    "nb_classifier = MultinomialNB()\n",
    "nb_classifier.fit(count_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We call predict with the count_vectorizer test data.\n",
    "predict will use the trained model to predict the label based on the test data vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# we save the predicted labels in variable pred to test the accuracy\n",
    "pred = nb_classifier.predict(count_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.934302751334\n"
     ]
    }
   ],
   "source": [
    "# testing accuracy\n",
    "print(metrics.accuracy_score(y_test, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[32920   751]\n",
      " [ 1761  2804]]\n",
      "\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "      False       0.95      0.98      0.96     33671\n",
      "       True       0.79      0.61      0.69      4565\n",
      "\n",
      "avg / total       0.93      0.93      0.93     38236\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# further evaluation of our model with confusion matrix which shows correct/incorrect labels\n",
    "print(metrics.confusion_matrix(y_test, pred, labels=[False, True]))\n",
    "print('\\n')\n",
    "print(metrics.classification_report(y_test, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
